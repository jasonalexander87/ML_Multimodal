{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για μοντέλο tflite σε JAVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για την περίπτωση του μοντέλου tensorflow που εκπαιδεύτηκε τα βήματα είναι τα εξής. Πρώτον δημιουργούμε τον converter και ορίζουμε τα \n",
    "optimizations,που θα εφαρμόσουμε. Πρόκειται για post trainning optimization το οποίο εφαρμόζει quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tensorflow.lite.TFLiteConverter.from_keras_model(model=model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "open(\"model.tflite\",\"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για την περίπτωση του tflite μοντέλου που μετατρέψαμε στο προηγούμενο δημιοργούμε ένα android app για να τρέξει.\n",
    "Αρχικά δημιουργούμε ένα αντικείμενο interpreter.Πρόκειται για deprecated API.Το πιό πρόσφατο είναι το Model.Στην συνέχεια φορτώνουμε το μοντέλο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite = new Interpreter(loadModelFile());\n",
    "\n",
    "private MappedByteBuffer loadModelFile() throws IOException {\n",
    "        AssetFileDescriptor fileDescriptor=this.getAssets().openFd(\"model.tflite\");\n",
    "        FileInputStream inputStream=new FileInputStream(fileDescriptor.getFileDescriptor());\n",
    "        FileChannel fileChannel=inputStream.getChannel();\n",
    "        long startOffset=fileDescriptor.getStartOffset();\n",
    "        long declareLength=fileDescriptor.getDeclaredLength();\n",
    "        return fileChannel.map(FileChannel.MapMode.READ_ONLY,startOffset,declareLength);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Και τέλος δημιουργόυμε τις input/output παραμέτρους και κάνουμε το inference step.Να σημειωθεί οτι πρόκειται για πολύ απλό μοντέλο όσον αφορά το \n",
    "input/output.Για πιό πολύπλοκα μοντέλα όπως πχ ένα που έχει είσοδο είκόνα μπορεί να χρησημοποιηθεί η tflite support library που περιέχει utilities που απλοποιούν την διαδικασία."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private float doInference(String inputString) {\n",
    "        float[] inputVal=new float[1];\n",
    "        inputVal[0]=Float.parseFloat(inputString);\n",
    "        float[][] output=new float[1][1];\n",
    "        tflite.run(inputVal,output);\n",
    "        return output[0][0];\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για την περίπτωση deployment με javascript έχουμε τα εξής βήματα. Αρχικά μετατρέπουμε το μοντέλο και εφαρμόζουμε optimizations όπως κάναμε και προηγουμένως."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflowjs.converters.save_keras_model(model, './')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στην συνέχεια δημιουργούμε μία σελίδα .html. Η σελίδα αυτή φορτώνει το javascript tensorflow καθώς και ένα script που θα κάνει το inference.Η εφαρμογή χρησημοποιεί WebView για να τρέξει html/javascript. Τα αρχεία json/bin του μοντέλου βρίσκονται ενσωματομένα στο apk και φορτώνονται στην WebView. Για να λειτουργήσει πρέπει να επιτρέψουμε την εκτέλεση javascript καθώς και να ενεργοποιήσουμε την πρόσβαση σε τοπικά αρχεία.Να σημειωθεί ότι αυτή η μέθοδος αφορά μόνο WebView.Σε κανονικό browser δεν λειτουργεί για λόγους ασφαλείας και ο μόνος τρόπος που μπορεί να λειτουργήσει είναι κάνοντας upload τα απαιτούμενα αρχεία μέσω html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const model = await tf.loadLayersModel(\"https://appassets.androidplatform.net/assets/model.json\");\n",
    "y = model.predict(tf.zeros([1]))\n",
    "document.getElementById(\"demo\").innerHTML = y; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για την περίπτωση της pytorch αρχικά μετατρέπουμε το μοντέλο και εφαρμόζουμε optimizations όπως περιγράφηκε στο report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_module = torch.jit.script(model)\n",
    "script_module_optimized = optimize_for_mobile(script_module)\n",
    "script_module_optimized._save_for_lite_interpreter(\"model.ptl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στην συνέχεια αφού εισάγουμε την βιβλιοθήκη φορτώνουμε το μοντέλο απο τα assets και δημιουργούμε ένα δείγμα εισόδου και παράγουμε την έξοδο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Module module = Module.load(assetFilePath(this, \"model.ptl\"));\n",
    "float[] a = new float[]{1.0f};\n",
    "Tensor input = Tensor.fromBlob(a,new long[]{1,1});\n",
    "Tensor outputTensor = module.forward(IValue.from(input)).toTensor();\n",
    "float[] scores = outputTensor.getDataAsFloatArray();"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
